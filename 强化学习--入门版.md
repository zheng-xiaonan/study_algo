
# 强化学习概要及经典算法讲解
## 概述
**强化学习 (Reinforcement learning, RL) **讨论的问题是**智能体** (agent) 如何在复杂、不确定的**环境**  (enviroment) 里去最大化它能获得的**奖励**。
![输入图片说明](/imgs/2024-12-03/5j73xZNETIM8wz9y.png)
组成部分：环境+智能体
智能体在环境中获得某个状态$S_t$ $\to$ 通过该状态输出一个动作（决策）$A_t$$\to$ 动作在环境中被执行，环境根据当前动作输出下一个状态$S_{t+1}$和当前状态带来的奖励$R_t$.
智能体的三种关键要素：感知、决策、奖励。
- **强化学习和监督学习的区别**
> 1. 样本层面：数据，数据iid,	且带有标签，
> 2. 优化目标层面：训练一个分类器，使loss降低来优化学习器；强化学习不是这样的。强化学习更多的是面向”一个序列“的学习，也就说可以理解一个基本的训练单元是一个序列。它最终是实现这个<S,A,R>序列的累积奖励最大化，而不是单个样本的奖励最大化。
> 3. 学习过程：监督学习通常是一次性学习过程，即通过一次训练过程学习模型；强化学习则是一个连续的决策过程，智能体需要不断与环境互动，通过试错来学习。

![输入图片说明](/imgs/2024-12-03/auG6qN6aXwMP11nI.png)


 - **相关术语**
	
	 - [ ] **策略（Policy）**
	策略是智能体用于决定下一步执行什么行动的规则。可以是确定性的，一般表示为：$\mu$:$$a_t=\mu(s_t)$$也可以是随机的，一般表示为$\pi$:$$a_t=\pi(·|s_t)$$

	 - [ ] **状态转移（State Transition）**
	 状态转移，可以是确定的也可以是随机的。可以用状态密度函数来表示：$$p(s_{t+1}|s_t,a_t) = P(S_{t+1}=s_{t+1}|S_t=s_t,A_t=a_t)$$在当前环境和行动下，衡量系统状态向某一个状态转移的概率。

	 - [ ] **回报（Return）**
	 从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的之和称为**回报**
	 $$G_t = R_t+R_{t+1}+R_{t+2}+\cdots=\sum_{k=0}^{\infty}R_{t+k}$$其中$R_t$表示第$t$时刻的奖励，agent的目标就是让Return最大化。
	 **折扣回报**
	 $$G_t = R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots = \sum_{k=0}^{\infty}\gamma^k R_{t+k}$$
	 $\gamma$为折扣率
	 

	 - [ ] **价值函数（Value Function）**
	 
	 - **状态价值函数**
	 一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function）:
		 ![输入图片说明](/imgs/2024-12-09/wvIgUuqpHt6rDvNu.png)
		 ![输入图片说明](/imgs/2024-12-09/wieNUJYQd4Xwoj2U.png)
	定义为从状态$s$出发遵循策略$\pi$能获得的期望回报.

	- **动作价值函数**
	 用$Q_\pi(s,a)$表示在遵循策略$\pi$时，对当前状态$s$执行动作$a$得到的期望回报：
	 ![输入图片说明](/imgs/2024-12-09/awQeXYgcuzYBmrF1.png)
	![输入图片说明](/imgs/2024-12-09/mBsEBh7dd8lnVHGm.png)

	状态价值函数和动作价值函数之间的关系：
	在使用策略$\pi$中，状态$s$的价值等于在该状态下基于策略$\pi$采取所有动作的概率与相应的价值相乘再求和的结果：$$V_\pi(s) = \sum_{a \in A}\pi(a|s)Q_\pi(s,a)$$ 使用策略$\pi$时，状态$s$下采取动作$a$的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：$$Q_\pi(s,a) = r(s,a)+\gamma\sum_{s^{'}\in S}P(s^{'}|s,a)V_\pi(s^{'})$$

$<S,A,P,r,\gamma>$

 - **强化学习大致分类**
> 1. model-free  VS model-based
> 2. on-policy VS off-policy
> 3. value-based VS policy-based

**免模型学习（Model-Free） vs 有模型学习（Model-Based）**
区分点之一就是 **智能体是否能完整了解或学习到所在环境的模型**。 环境的模型是指一个预测状态转换和奖励函数。
![输入图片说明](/imgs/2024-12-08/IhUEvYGq5HGB091s.png)

*一般情况下，环境都是不可知的，也可以不必去学习所在的环境模型；所以主要研究无模型model-free问题。*

**在线策略（On-Policy） vs 离线策略（Off-Policy）**
按照学习方式进行划分的；
-   On-Policy是指agent必须本人在场，并且一定是本人边玩边学习。典型的算法为 Sarsa。
-   ﻿Off-Policy是指agent可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习，也没有必要是边玩边学习，玩和学习的时间可以不同步。典型的方法是Q-learning， 以及Deep-Q-Network。

**基于策略（Policy-Based）和基于价值（Value-Based）**
按照学习目标划分；
![输入图片说明](/imgs/2024-12-08/MPkQWmJpyIBowF8n.png)
-   ﻿policy-Based的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy-gradients.
-   ﻿﻿value-Based的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有Q-learning、Deep Q Network和Sarsa。
-   ﻿二者的结合：Actor-Critic（演员-评论家算法）， Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程，常见的有DDPG等。

## 马尔可夫决策过程（MDP）
![输入图片说明](/imgs/2024-12-09/9Qbg20wcrfDJQ95v.png)
MDP 可以由五元组表示$<S,A,\{p_{sa}\},\gamma, R>$
 - **S（状态）**：智能体**观察**到的当前环境的**部分或者全部特征**
							**状态空间**就是智能体能够观察到的特征数量
 - **A（动作）**：智能体做出的具体行为
**动作空间**就是该智能体能够做出的动作数量。
 - **$P_{sa}$** 是状态转移概率
	 
 - **$\gamma$** 是折扣因子
 -  **R（reward）奖励**：当我们在某个状态下，完成动作。环境就会给我们反馈，告诉我们这个动作的效果如何。这种效果的数值表达，就是**奖励**
 - 马尔可夫的不确定性
![输入图片说明](/imgs/2024-12-09/rFWIjcNly7DNUbhR.png)
#### Q值和V值
 
 - 评估**状态**的价值，我们称为**V值**：它代表了智能体在这个状态下，一直到最终状态的**奖励总和**的**期望**。
	![输入图片说明](/imgs/2024-12-09/XIrRsp0PVk8BA3l6.png)
		  **V值是会根据不同的策略有所变化的！**
 
 - 评估**动作**的价值，我们称为**Q值**：它代表了智能体选择这个动作后，一直到最终状态**奖励总和**的**期望**；
  ![输入图片说明](/imgs/2024-12-09/4XrEieEsYT3GvMVX.png)
与V值不同，Q值和策略并没有直接相关，而与环境的状态转移概率相关。

Q值和V值的意义相通的： 1. 都是马尔可夫决策过程的节点； 2. 价值评价的方式是一样的： - 从当前节点出发 - 一直走到最终节点 - 所有的奖励的期望值;
Q-V的关系图：
![输入图片说明](/imgs/2024-12-09/sLXI5brDQxchMtuq.png)
![输入图片说明](/imgs/2024-12-09/tZn1pXuoBjwwHHWj.png)
![输入图片说明](/imgs/2024-12-09/R2s8sOPa8AXvmRAl.png)
![输入图片说明](/imgs/2024-12-09/dCxdWftSfCjmvJGN.png)
$V \to V$
![输入图片说明](/imgs/2024-12-09/8i4rCPVCBUIrezC0.png)

动态规划求解简单的白盒强化学习任务。

回顾强化学习的目的是什么？--决策
那么既然要决策了，有两种方式可以让我们做出决策动作：
1.  我们知道每个状态的价值，比如当我们处于A状态时，我们知道下一时刻可能去往的状态B、C、D...等等各自的价值（称为状态价值函数，V值），那么我们每一次都做出能够到达最高价值的状态的动作，利用贪心思想，不就可以得到全局的最优解吗？
2.  我们不知道每个状态的价值，但是我们知道在每个状态下，每个动作各自的价值，那么当处于状态A时，我们采取能够带来最大价值的动作（称为动作价值函数，Q值），再次利用贪心的思想，不也就得到全局的最优解吗？

所以大部分的强化学习无非就是在做两个工作，要么是预测每个状态的价值（称为状态价值函数，V值），要么是预测在每个状态下，所有可能出现的动作的价值（称为动作价值函数，Q值），进而实现决策。在每一种强化学习算法中，这两个工作可能是一起进行的，也有可能在某个算法里只进行其中一个工作，比如只预测每个状态的价值或者只预测在每个状态下，所有可能出现的动作的价值（也有例外，PG算法）。
![输入图片说明](/imgs/2024-12-09/Z5pbXDCwhaFKoT60.png)

## 蒙特卡洛算法（MC)

蒙特卡洛算法做的就是**第一个工作**，即要把每个状态的价值（称为状态价值函数，V值）给预测出来。
蒙特卡洛原理：大量重复随机采样来获取某个数值结果，（大数定律）
**算法原理**
![输入图片说明](/imgs/2024-12-10/5TM3GVk7KguEHbWQ.png)
![输入图片说明](/imgs/2024-12-10/aRg6LMKSlRWnm74X.png)

## 时序差分算法（TD)
![](https://staticcdn.boyuai.com/comment/upload/M8quD9HOCno3USQDX2J1E/502/2020/07/26/U9cxb5Hv-Kh7zrZ4-uEY1.jpg)

## SARSA（状态-动作-奖励-状态-动作）
根据TD的思想：预估状态价值的迭代公式：$$V(S_t) \leftarrow V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]$$借鉴 **更新状态价值**的思想同样去**估计动作价值**，也就是上文提到的第二条思路；
> **$\epsilon - greedy$策略**
> 如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对$(s,a)$永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。简单常用的解决方案是不再一味使用贪婪算法，而是采用一个$\epsilon - greedy$策略
> ![输入图片说明](/imgs/2024-12-12/2fkkUytW02vsFur9.png)
> $\epsilon$可以设置为定值，也可以令$\epsilon$随时间衰减。

![输入图片说明](/imgs/2024-12-12/1orvtSQ5K5pvhi7C.png)

不需要同时更新$Q,V$, 使用下一个动作的$Q$值代替$V$， 其中一种替代方案为：
**使用同一个策略下产生的动作$A$的$Q$值替代$V(S_{t+1})$** ---**SARSA--On Policy**
$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$$

![输入图片说明](/imgs/2024-12-12/iADDaaQBGVFx8Ho3.png)
![输入图片说明](/imgs/2024-12-12/x8WE7D9xuVQIce9n.png)![](https://staticcdn.boyuai.com/comment/upload/ma6SNRWrOJ3U2wtEL5b_4/398/2020/07/25/5n5LMZL5hhZEP3RV58aXa.jpg)
多步SARSA算法参考多步TD的思想；
考虑：有没有别的替代方式？

## Q-learning
在更新$Q(S,A)$的时候，不使用同一个$\epsilon - greedy$策略，而是一种更激进的方式来更新--贪心策略，**每次更新的时候使用下一步最大奖赏的动作**
$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma  \max _{A_{t+1}} Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$$ 

![输入图片说明](/imgs/2024-12-12/OaXxWfdmwTb34GV5.png)
此时行动策略和更新策略不是同一个策略， 称为**Off Policy**

> Q-table（Q表格）
> 在进行Q值更新之前，会初始化Q表格来存储各个状态的及各个状态的Q值，其中表纵坐标是状态，横坐标是在这个状态下的动作。
> **适用情况：状态、动作空间离散且有限；**

对于连续型状态，或者状态、动作空间非常大的时候，使用上述的方法基于Q表格的Q-learning算法无法解决；这时候看看考虑引入神经网络来解决这类问题；
Q表格的作用是在Q值更新过程中，给定状态$S$，可以通过查表的方式获得最大Q值的动作A,即$S-A$的对应关系，可以用一个函数来表示$S-A$的对应关系。

## DQN

 - DQN = Deep learning + Q-learning
 - DQN的主要思想就是使用一个**函数（深度神经网络）来代替原来的Q-table**；
 - [ ] 更新规则

$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma  \max _{A_{t+1}} Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$$ 

 - [ ] Q网络的损失函数：
 更新规则中使用时序差分学习目标$R_{t+1}+\gamma \max _{A_{t+1}} Q(S_{t+1},A_{t+1})$来增量式更新$Q(S_t,A_t)$,即更新目标是使两者靠近，因此对于一组数据$\{(s_i,a_i,r_i,s_i^{'})\}$,我们可以很自然地将 Q 网络的损失函数构造为均方误差的形式：![输入图片说明](/imgs/2024-12-15/dl1OXuqvSmykn2s4.png)
 至此，我们就可以将 Q-learning 扩展到神经网络形式——**深度 Q 网络**（deep Q network，DQN）算法。
 - [ ] 经验回放
按照TD的算法思路，每次更新完一次$w$后，不会再使用本次的transition $(s_t, a_t, r_t, s_{t+1})$*（一条训练数据）*,存在经验浪费的缺点；另外前后两个 transition 之间存在很强的关联。会造成训练的样本之间存在很强的相关性，不利于神经网络的训练；经验回放的思想可以很好的解决这两个问题。
为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了**经验回放**（experience replay）方法，具体做法为维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。
> 一个改进：优先经验回放 ：不在使用均匀采样的思想，根据 transition 样本的重要性进行加权采样。
 - [ ] 目标网络
 DQN 算法最终更新的目标是让$Q_w(s,a)$逼近$r+\gamma max_a Q_w(s',a')$，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN 便使用了**目标网络**（target network）的思想：既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住。为了实现这一思想，我们需要利用两套 Q 网络。
	 1. 训练网络
	 训练网络$Q_w(s,a)$用于计算原来损失函数中的$Q_w(s,a)$项，并且使用正常梯度下降方法来进行更新。
	 2. 目标网络
	 目标网络$Q_{w^-}(s,a)$用于计算原来损失函数中的$(r+\gamma max_a Q_w(s',a'))$其中$w^{-}$表示目标网络中的参数。如果两套网络的参数随时保持一致，则仍为原先不够稳定的算法。为了让更新目标更稳定，目标网络并不会每一步都更新。具体而言，目标网络使用训练网络的一套较旧的参数，训练网络$Q_w(s,a)$在训练中的每一步都会更新，而目标网络的参数每$C$隔步才会与训练网络同步一次，即$w \to w^{-}$。这样做使得目标网络相对于训练网络更加稳定。
	 ![输入图片说明](/imgs/2024-12-16/jOj271gugSqJv7fp.png)
 - [ ] DQN算法流程
![输入图片说明](/imgs/2024-12-16/Q2y2HVPyLvCJl6nm.png)

## DQN改进算法
### Double DQN
普通的 DQN 算法通常会导致对$Q$值的过高估（overestimation）。可以对传统DQN优化的TD误差目标进行分析。

基于DQN在目标网络的改进：
为了解决对$Q$值的过高估的问题，Double DQN 算法提出利用两个独立训练的神经网络估算$\max_{a^{'}}Q_*(s',a')$。具体做法是将原有的$\max_{a^{'}} Q_{w^-}(s',a')$更改为$Q_{w^-}(s',arg\max_{a^{'}} Q_{w}(s',a'))$，即利用一套神经网络$Q_w$的输出选取价值最大的动作，但在使用该动作的价值时，用另一套神经网络$Q_{w^-}$计算该动作的价值。这样，即使其中一套神经网络的某个动作存在比较严重的过高估计问题，由于另一套神经网络的存在，这个动作最终使用的值不会存在很大的过高估计问题。

在传统的 DQN 算法中，本来就存在两套函数的神经网络——目标网络和训练网络，只不过$\max_{a^{'}} Q_{w^-}(s',a')$的计算只用到了其中的目标网络，那么我们恰好可以直接将训练网络作为 Double DQN 算法中的第一套神经网络来选取动作，将目标网络作为第二套神经网络计算值，这便是 Double DQN 的主要思想。因此基于DQN的网络参数形式可以直接写出Double DQN的优化目标：
$$r+\gamma Q_{w^-}(s',\argmax_{a'}Q_w(s',a'))$$

### Dueling DQN
Q-table、DQN中的Q神经网络都是直接对Q值的估计，而Dueling DQN的思想是将Q值的估计拆分成两个值的估计$S$值和$A$值。
S值可以看成是该state下的Q值的平均数----状态价值函数。A值是有所限制的，A值的平均数为0，叫做优势函数，满足$$Q(s,a)=A(s,a)+V(s)$$因此在Dueling DQN中，Q网络被建模为：$$Q_{\eta,\alpha,\beta}(s,a) = V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)$$其中，$V_{\eta,\alpha}(s)$为状态价值函数，而$A_{\eta,\beta}(s,a)$则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性；$\eta$是状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层；而$\alpha$和$\beta$分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到值。
![输入图片说明](/imgs/2024-12-16/dtH4ZjFZYXOLpiJ9.png)
> 原始的DQN, 在每次更新过程中，提升的是当前状态下的某个动作的Q值，而Dueling DQN： 在网络更新的时候，由于有**A值之和必须为0**的限制，所以网络会**优先更新S值**。S值是Q值的平均数，平均数的调整相当于一次性S下的所有Q值都更新一遍。所以网络在更新的时候，不但更新某个动作的Q值，而是把这个状态下，所有动作的Q值都调整一次。这样，就可以在更少的次数让更多的值进行更新。使得学习效率更高效。
> 在DuelingDQN，我们只是**优先**调整S值。但最终我们的target目标是没有变的，所以我们最后更新出来结果Q值也是对的。
相比于DQN、DDQN，只是改变了网络结构。其余的没变化。

到目前为止，我们学习的强化学习都在干一件事，就是努力的去求出各个状态的价值（V）或者各个状态下每个动作的价值（Q），为什么一定要这样做呢？对于每一个状态，我们直接算出每一个动作可能出现的概率不行吗？然后直接把概率最大的动作作为当前状态应该采取的动作不就行了吗？这样不就可以跳过求解V和Q这两个值了。
value-based、policy-based
## 策略梯度（Policy Gradient）
![输入图片说明](/imgs/2024-12-16/dQiTYsNx0n6VaSLI.png)
![输入图片说明](/imgs/2024-12-16/U47A1qS4xAvWHoUu.png)
想法：直接从输入得到最优策略，那么需要思考两个问题：1. 如何评价一个策略的好与坏？2. 如何搜索最优策略？

>1：将策略参数化，使用一个目标函数来定义最优策略
>$$J(\theta) = E_S[V_{\pi_\theta} (S)]$$
>状态价值函数$V_\pi(s_t) = E_{A_t \sim \pi(\bullet|s_t;\theta)}[Q_\pi(s_t,A_t)]$，$J(\theta)$表示平均状态价值；
>策略学习可以描述为优化问题：$\max J(\theta)$

> 2. 有了目标函数，我们将目标函数对策略$\theta$求导，得到导数后，就可以用梯度上升方法$\theta=\theta+\alpha\triangledown_\theta J(\theta)$来最大化这个目标函数，从而得到最优策略。
![输入图片说明](/imgs/2024-12-16/ajk8fRUISKS7GiOz.png)
注意：因为上式中期望$E$的下标是$\pi_\theta$，所以策略梯度算法为在线策略（on-policy）算法,即必须使用当前策略$\pi_\theta$采样得到的数据来计算梯度。
可以发现在每一个状态下，梯度的修改是让策略更多地去采样到带来较高Q值的动作，更少地去采样到带来较低Q值的动作;

在计算策略梯度的公式中，我们需要用到$Q_{\pi_\theta}(s,a)$，可以用多种方式对它进行估计。

 - [ ] **REINFORCE 算法**
 $Q_{\pi_\theta}(s,a)$通过MC的方法来估计，即使用累计奖励值$G_t$作为 $Q_{\pi_\theta}(s,a)$ 的无偏估计, 对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为：
![输入图片说明](/imgs/2024-12-16/kJK8GbEXKUuoMBu2.png)

![输入图片说明](/imgs/2024-12-16/HE9pXTKWpUftJFX3.png)
-   **REINFORCE**是Monte-Carlo式的探索更新，也就是回合制的更新，至少要等一个回合结束才能更新policy;
- **REINFORCE**是在线策略算法，之前收集到的轨迹数据不会被再次利用。因此该算法需要使用更多的采样序列；
- **REINFORCE** 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。（由于蒙特卡洛的估计的方差较大）


## **Actor-Critic 算法（演员-评论家算法）**

**REINFORCE**是Monte-Carlo式的探索更新，必须需要完整的游戏过程，直到最终状态，才能通过回溯计算G值。因此可以考虑使用TD式的探索更新，另外，在PG，我们需要计算G值；那么在TD中，我们应该怎样估算每一步的Q值呢？借鉴DQN的思想，使用神经网络来解决。

> Actor-Critic思想，其实是用了两个网络：
> 两个网络有一个共同点，输入状态S: 一个输出策略，负责选择动作，我们把这个网络成为Actor；
 > 一个负责计算每个动作的分数，我们把这个网络成为Critic。

在策略梯度中，可以把梯度写成下面这个更加一般的形式（加权的梯度更新策略）：
![输入图片说明](/imgs/2024-12-16/FjQExyG4m1SiNYpN.png)
    Actor 的更新采用策略梯度的原则，见PG;
    分析 Critic 如何更新？
   根据上述分析Critic由原来的预估Q值转换成预估V值，那么，这个TD-error是用来更新Critic的loss了！因此Critic的任务就是让TD-error尽量小。然后TD-error给Actor做更新。
   Critic价值网络表示为$V_w$，参数为$w$, 那么对于单个数据定义价值的损失函数可以表示为：   ![输入图片说明](/imgs/2024-12-16/VqCzcajFQot5X6qr.png)
   然后使用梯度下降方法来更新 Critic 价值网络参数即可。
   ![输入图片说明](/imgs/2024-12-16/ydcEZ6NZ9gUEFJXa.png)
   ![输入图片说明](/imgs/2024-12-16/P14xCAhtOsyIS9qt.png)
 基于AC的一些改进算法：TRPO算法、PPO算法
 ## DDPG (deep deterministic policy gradient 深度确定性策略梯度算法)
 之前的策略性算法输出的都是动作集合的概率分布，然后根据分布去采样某个动作，属于随机性的策略算法，DDPG直接输出一个动作而非概率。
 DDPG 同时包含AC和DQN的思想；
 DQN：![输入图片说明](/imgs/2024-12-17/U8Jaw9qzkUTeICfj.png)
 考虑DQN不能处理连续动作的原因？
  
  DQN -- 使用神经网络解决了Qlearning不能解决的连续状态空间问题，同意可以考虑神经网络解决连续动作的问题。
  在DDPG中，用一个神经网络，直接替代$\max Q(s',a')$的功能。输入状态s，函数返回动作action的取值，这个取值能够让$Q$值最大。这个就是DDPG中的Actor的功能。Critic与AC中的critic网络类似，只是不是预估$V$值，而是预估$Q$值。
  $$\mu(s) = \argmax_aQ(s,a)$$式中的$Q$就是Critic, $\mu$就是Actor;
  ![输入图片说明](/imgs/2024-12-17/GmkIQ4Me9lwipfVa.png)
  
  > Critic
  > -   Critic网络的作用是预估Q，虽然它还叫Critic，但和AC中的Critic不一样，这里预估的是Q不是V；
> -   注意Critic的输入有两个：动作和状态，需要一起输入到Critic中；
> - Critic网络的loss其还是和AC一样，用的是TD-error。

> Actor
> -   和AC不同，Actor输出的是一个动作；
> -   Actor的功能是，输出一个动作A，这个动作A输入到Crititc后，能够获得最大的Q值。

**确定性策略梯度定理**
![输入图片说明](/imgs/2024-12-17/SZl7x5Cp22dWPArI.png)
首先$Q$用$\mu_\theta$对求导$\triangledown_\theta Q(s,\mu_\theta(s))$，其中会用到梯度的链式法则，先对$a$求导，再对$\theta$求导。然后通过梯度上升的方法来最大化函数$Q$，得到$Q$值最大的动作。

DDPG也采用目标网络的方式，就是先冻结住用来求target的网络。在更新之后，再把参数赋值到target网络。目标网络的更新方式与DQN有所不同：
![输入图片说明](/imgs/2024-12-17/yEWnTrGSHLsLLMhX.png)

对于DQN的优化手段也可以用在DDPG上，如DDQN.
![输入图片说明](/imgs/2024-12-17/Y2MJ04sNjXSNlSTL.png)

